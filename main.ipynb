{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b8625db0-cfe6-4c44-abfd-67a5fb134051",
   "metadata": {},
   "source": [
    "# Information Extraction from Electricity Invoices\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "94d449dc-924d-4357-8567-7c7bed016258",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, sys, pickle, time, uuid, shutil\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from ast import literal_eval\n",
    "from joblib import dump, load\n",
    "import re, subprocess\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sn\n",
    "\n",
    "# Constant definitions\n",
    "NWORDS = 11 #number of words per training sentence\n",
    "NCF = 5     #number of words for calculating custom features (maximum=NWORDS/2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af08be85-e164-4dda-aa80-a9354ef22c79",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Code for converting part of the training/test set into our (sentence,label) format\n",
    "\n",
    "from idsem2list import idsem_to_list\n",
    "\n",
    "generate_train = True\n",
    "nbills = 100\n",
    "\n",
    "# List template directories\n",
    "if generate_train:\n",
    "    base_dir = \"../dataset/idsem_txt/training/\"\n",
    "else:\n",
    "    base_dir = \"../dataset/idsem_txt/test/\"\n",
    "\n",
    "directories = os.listdir(base_dir)\n",
    "\n",
    "if generate_train:\n",
    "    out_dir = \"train_files_\" + str(nbills)\n",
    "else:\n",
    "    out_dir = \"test_files_\" + str(nbills)\n",
    "\n",
    "ext = 0\n",
    "for directory in directories:\n",
    "    ext = ext+1\n",
    "    train_data = idsem_to_list(base_dir+directory, nbills, out_dir, ext)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "cbb71865-5f51-436d-966e-dd09a3ab257e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Use label codes for indexing the DataFrame\n",
    "label_codes = literal_eval(open(\"dictionaries/label_codes.txt\", mode=\"r\", encoding=\"utf-8\").read())\n",
    "\n",
    "# Create inverse mapping of label codes \n",
    "code_labels = {i: c for c,i in label_codes.items()}\n",
    "\n",
    "# Read spanish stop words\n",
    "stop_words = literal_eval(open(\"dictionaries/spanish_stop_words.txt\", mode=\"r\", encoding=\"utf-8\").read())\n",
    "stop_words = list(stop_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "23d4f93b-eff2-4b56-a765-7fa2abcf753a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "\n",
    "class WordVectorizer(TransformerMixin, BaseEstimator):\n",
    "   \"\"\"Extract features from the central word and its context\"\"\"\n",
    "   \n",
    "   def __init__(self, NCF=5):\n",
    "      self.NCF = NCF\n",
    "   \n",
    "   def word_type(self, S): \n",
    "      \"\"\"WordNType: alphabetic, numeric, alphanumeric, numericcomma\"\"\"\n",
    "      f = pd.Series(0, index=S.index)\n",
    "      f[S.str.isalnum()==True] = 1\n",
    "      f[S.str.isalpha()==True] = 2\n",
    "      f[S.str.isdigit()==True] = 3\n",
    "      f[S.str.contains('\\b[0-9,.]+$', regex=True)==True] = 4\n",
    "      return f\n",
    "   \n",
    "   def word_pretype(self, S): \n",
    "      \"\"\"WordNPretype: money, DNI, email, web, postcode\"\"\"\n",
    "      money = r'^\\d+[,.]\\d\\d$'\n",
    "      email = r\"^[a-zA-Z0-9.!#$%&'*+/=?^_`{|}~-]+@[a-zA-Z0-9](?:[a-zA-Z0-9-]{0,61}\"\\\n",
    "              \"[a-zA-Z0-9])?(?:\\.[a-zA-Z0-9](?:[a-zA-Z0-9-]{0,61}[a-zA-Z0-9])?)*$\"\n",
    "      web   = r\"^(https?:\\/\\/)?(www\\.)?([a-zA-Z0-9]+(-?[a-zA-Z0-9])*\\.)+[\\w]{2,}(\\/\\S*)?$\"\n",
    "      web2  = r\"^(http:\\/\\/www\\.|https:\\/\\/www\\.|http:\\/\\/|https:\\/\\/)?[a-z0-9]+([\\-\\.]{1}[a-z0-9]+)*\\.[a-z]{2,5}(:[0-9]{1,5})?(\\/.*)?$\"\n",
    "\n",
    "      dni1  = r'^[klmxyzKLMXYZ][-]?\\d{7}[-]?[a-zA-Z]$'\n",
    "      dni2  = r'^\\d{8}[-]?[a-zA-Z]$'\n",
    "      pcode = r\"^[0-5]\\d{4}$\"\n",
    "      date  = r\"^\\d\\d[/\\-.]\\d\\d[/\\-.]\\d{4}$\"\n",
    "      \n",
    "      f = pd.Series(0, index=S.index)\n",
    "      f[S.str.contains(money, regex=True)==True] = 1\n",
    "      f[S.str.contains(dni1,  regex=True)==True] = 2\n",
    "      f[S.str.contains(dni2,  regex=True)==True] = 2\n",
    "      f[S.str.contains(email, regex=True)==True] = 3\n",
    "      f[S.str.contains(web2,   regex=True)==True] = 4\n",
    "      f[S.str.contains(pcode, regex=True)==True] = 5\n",
    "      f[S.str.contains(date,  regex=True)==True] = 6\n",
    "      return f\n",
    "      \n",
    "   def word_mesure(self, S): \n",
    "      \"\"\"WordNMeasure: euro, euroday, eurokw, eurokwh, \n",
    "         kw, kwday, kwmonth, kwhour, %\"\"\"\n",
    "         \n",
    "      measures = {'€':1,      '€/día': 2,    '€/kw': 3,     '€/kwh': 4, \n",
    "                  'eur': 1,   'eur/día': 2,  'eur/kw': 3,   'eur/kwh': 4,\n",
    "                  'euros': 1, 'euros/día': 2,'euros/kw': 3, 'euros/kwh': 4,\n",
    "                  'kw': 6,    'kw/mes': 7,   'kwh': 8,      '%': 9}\n",
    "      L = S.str.lower()\n",
    "      f = L.map(measures)\n",
    "      f.fillna(0, inplace = True)\n",
    "      return f\n",
    "\n",
    "   def word_capital(self, S): \n",
    "      \"WordNCapital: firstlettercapital, onewordcapital, allcapital, nocapital\"\n",
    "      f = pd.Series(0, index=S.index) \n",
    "      f[S.str.contains(\"^[A-Z][a-z]+$\", regex=True)==True] = 1\n",
    "      f[S.str.contains(\"^[A-Z]$\", regex=True)==True] = 2\n",
    "      f[S.str.isupper()==True] = 3\n",
    "      f[S.str.islower()==True] = 4\n",
    "      f[S.str.istitle()==True] = 5\n",
    "      return f\n",
    " \n",
    "   def word_colons(self, S):    \n",
    "      \"WordNBeforeColons: 1 if it is followed by a semicolon or 0 otherwise\"\n",
    "      f = pd.Series(0, index=S.index) \n",
    "      f[S.str.contains(\":\")==True] = 1\n",
    "      f[S.str.contains(\";\")==True] = 2\n",
    "      f[S.str.contains(\".\")==True] = 3\n",
    "      return f\n",
    "\n",
    "   def fit(self, X, y=None):\n",
    "      return self\n",
    "\n",
    "   def transform(self, X, y=None):\n",
    "      \"\"\" Create custom features for a given word column in DataFrame\"\"\"\n",
    "      W = pd.DataFrame()\n",
    "      \n",
    "      if True:\n",
    "         # Use all the words in the training sentence\n",
    "         F = pd.DataFrame([t.split() for t in X], index = X.index)\n",
    "\n",
    "         print(\"NCF: \", self.NCF)\n",
    "         # Convert center word \n",
    "         if self.NCF>=0:\n",
    "            W[\"Type0\"]  = self.word_type(F[5])\n",
    "            W[\"PreType0\"]  = self.word_pretype(F[5])\n",
    "            W[\"Measure0\"]  = self.word_mesure(F[5])\n",
    "            W[\"Capital0\"]  = self.word_capital(F[5])\n",
    "            W[\"Colons0\"]  = self.word_colons(F[5])\n",
    "            \n",
    "            for i in range(self.NCF):\n",
    "               # Convert previous words\n",
    "               W[\"Type-\"+str(i+1)]  = self.word_type(F[5-i-1])\n",
    "               W[\"PreType-\"+str(i+1)]  = self.word_pretype(F[5-i-1])\n",
    "               W[\"Measure-\"+str(i+1)]  = self.word_mesure(F[5-i-1])\n",
    "               W[\"Capital-\"+str(i+1)]  = self.word_capital(F[5-i-1])\n",
    "               W[\"Colons-\"+str(i+1)]  = self.word_colons(F[5-i-1])\n",
    "               # Convert following words\n",
    "               W[\"Type+\"+str(i+1)]  = self.word_type(F[5+i+1])\n",
    "               W[\"PreType+\"+str(i+1)]  = self.word_pretype(F[5+i+1])\n",
    "               W[\"Measure+\"+str(i+1)]  = self.word_mesure(F[5+i+1])\n",
    "               W[\"Capital+\"+str(i+1)]  = self.word_capital(F[5+i+1])\n",
    "               W[\"Colons+\"+str(i+1)]  = self.word_colons(F[5+i+1])\n",
    "\n",
    "      return W\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9edc85fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import RidgeClassifier, LogisticRegression\n",
    "from sklearn.gaussian_process import GaussianProcessClassifier\n",
    "from sklearn.gaussian_process.kernels import RBF\n",
    "from sklearn.naive_bayes import MultinomialNB, GaussianNB\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.svm import SVC, LinearSVC\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import AdaBoostClassifier, RandomForestClassifier\n",
    "from sklearn.pipeline import Pipeline, FeatureUnion\n",
    "\n",
    "# Models \n",
    "def create_model(type):\n",
    "   \"\"\" Possible Models \"\"\"\n",
    "   model ={\n",
    "      \"logistic\": LogisticRegression(C=5, solver=\"saga\", tol=0.01),\n",
    "      \"nbayes\": MultinomialNB(alpha=0.01),\n",
    "      \"svmrbf\": SVC(gamma=0.001, C=100.0),\n",
    "      \"svmlin\": LinearSVC(random_state=0, class_weight=\"balanced\", tol=1e-5),\n",
    "      \"decisiontree\": DecisionTreeClassifier(random_state=42),\n",
    "      \"randomforest\": RandomForestClassifier(n_estimators=6, random_state=42, n_jobs=-1)\n",
    "   }\n",
    "\n",
    "   return model[type]\n",
    "\n",
    "all_models=[\"logistic\", \"nbayes\", \"svmrbf\", \"svmlin\", \n",
    "            \"decisiontree\", \"randomforest\"]\n",
    "\n",
    "# models for testing\n",
    "models=[\"logistic\", \"nbayes\"] \n",
    "        #\"svmlin\", \"decisiontree\", \"randomforest\", \"svmrbf\" ]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ff52e25-6116-4e93-afc7-ba0010cbdb90",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def process_input_file(file_bill, verbose=False):\n",
    "    \"\"\"Function that converts the PDF or image file to TXT\"\"\"\n",
    "\n",
    "    file_txt = 'tmp/' + str(uuid.uuid4())\n",
    "\n",
    "    file_type = os.path.splitext(file_bill)[1][1:].lower()\n",
    "\n",
    "    # convert input file to txt\n",
    "    if file_type == 'pdf':\n",
    "        #convert pdf to text\n",
    "        if verbose: print(\"Reading pdf file...\")\n",
    "        with open(file_bill,  \"rb\") as f:\n",
    "           pdf = pdftotext.PDF(f)\n",
    "        if verbose: print(\"Number of pages: \", len(pdf))\n",
    "\n",
    "        fd=open(file_txt + '.txt', \"w\");\n",
    "\n",
    "        if verbose: print(\"Converting to txt in \", file_txt + '.txt')\n",
    "\n",
    "        fd.write(\"\\n\".join(pdf))\n",
    "        fd.close()\n",
    "    else:\n",
    "        if file_type == 'txt':\n",
    "            #if a txt file just copy to destiny\n",
    "            shutil.copyfile(file_bill, file_txt + '.txt')\n",
    "        else:\n",
    "            #convert image to text\n",
    "            subprocess.call(['tesseract', file_bill, file_txt,\n",
    "                             '--oem', '1', '-l', 'spa'])\n",
    "\n",
    "    file_txt = file_txt + '.txt'\n",
    "    \n",
    "    lines = open(file_txt, 'r').readlines()\n",
    "    \n",
    "    os.remove(file_txt)\n",
    "    \n",
    "    return lines\n",
    "\n",
    "\n",
    "def read_file(filename, verbose=False):\n",
    "   \"\"\"\n",
    "   Read the content of one file\n",
    "   \"\"\"\n",
    "   l=[]\n",
    "   if verbose: print(\"Reading file \", filename)\n",
    "   with open(filename,'r', encoding=\"utf-8\") as f: \n",
    "         l=literal_eval(f.read())\n",
    "   return l\n",
    "   \n",
    "\n",
    "def read_directory(directory, verbose=False):\n",
    "   \"\"\"\n",
    "   Read the files in a directory and\n",
    "   inserts the data in a list\n",
    "   \"\"\"\n",
    "   X = []\n",
    "   if verbose: print(\"Accessing directory \", directory)\n",
    "   for filename in os.listdir(directory):\n",
    "      l=read_file(directory+\"/\"+filename, verbose)\n",
    "      X += l\n",
    "         \n",
    "   if verbose: print('Training data contains ', len(X), ' samples')\n",
    "   \n",
    "   return X\n",
    "\n",
    "\n",
    "def create_dataframe(data):\n",
    "   \"\"\"\n",
    "   Function to create a DataFrame from a text training set \n",
    "   It processes word and line features\n",
    "   In this case there is only one label per line\n",
    "   \"\"\"\n",
    "   # Add columns for the text and each label in the DataFrame\n",
    "   F = pd.DataFrame(data, columns = [\"Text\", \"Label\"])\n",
    "\n",
    "   #extract the center and next words\n",
    "   a = F[\"Text\"].str.split(expand=True)\n",
    "   F[\"Word\"] = a[5]\n",
    "   F[\"NextWord\"] = a[6]\n",
    "\n",
    "   # remove # from the name of the labels\n",
    "   F[\"Label\"] = F[\"Label\"].apply(lambda x: x[1:])\n",
    "   F.index.names = ['Id']\n",
    "\n",
    "   return F\n",
    "\n",
    "\n",
    "def read_data(filename, no_split, is_directory=True, verbose=False):\n",
    "   ''' Read the data from the training dataset file\n",
    "       Separate the features from the labels\n",
    "       Separate dataset in train and validation sets\n",
    "   '''\n",
    "   \n",
    "   # Read the DataFrame as Text + Label\n",
    "   if is_directory:\n",
    "      X = read_directory(filename, verbose)\n",
    "   else:\n",
    "      X = read_file(filename, verbose)\n",
    "        \n",
    "   # Read DataFrame with one Label column\n",
    "   X = create_dataframe(X)\n",
    "\n",
    "   # Number and percentage of labels  \n",
    "   n = X['Label'].value_counts()\n",
    "   S = n.sum()\n",
    "\n",
    "   # Take out the label column and transform label names into codes\n",
    "   y = pd.DataFrame(X[\"Label\"], columns = [\"Label\"])\n",
    "   n = y.value_counts()\n",
    "   n[0:] *= 100\n",
    "   f = open(\"count-labels.txt\", \"w\")\n",
    "   f.write(n.to_string())\n",
    "   f.close()\n",
    "    \n",
    "   y = y.applymap(lambda x: label_codes[x]) \n",
    "      \n",
    "   # Remove label from feature set DataFrame\n",
    "   X.drop(\"Label\", axis=1, inplace=True)\n",
    "\n",
    "   # Encode labels with dictionary codes\n",
    "   y.index.rename('Id', inplace = True)\n",
    "   \n",
    "   if no_split:\n",
    "      return X, y\n",
    "   else:\n",
    "      X_train, X_valid, y_train, y_valid = train_test_split(\n",
    "         X, y, train_size=0.8, test_size=0.2, random_state=0, \n",
    "         shuffle=True\n",
    "      )\n",
    "\n",
    "      return X_train, X_valid, y_train, y_valid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1917f337",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import metrics\n",
    "\n",
    "def evaluate(clf, X, y, outputfile=None):\n",
    "   \"\"\" Evaluate the performance of a model with the validation/test set \"\"\"\n",
    "\n",
    "   # Preprocessing of validation data, get predictions\n",
    "   predicted = clf.predict(X.Text)\n",
    "   mae = metrics.mean_absolute_error(y, predicted)\n",
    "   print('  - MAE:', mae)\n",
    "\n",
    "   #metrics precision recall f1-score support\n",
    "   labels = y.unique()\n",
    "   index = [code_labels[i] for i in labels]\n",
    "\n",
    "   clr = metrics.classification_report(y, predicted,\n",
    "         labels=labels, target_names = index, digits=4)\n",
    "\n",
    "   labels=list(code_labels.keys())\n",
    "   cm = metrics.confusion_matrix(y, predicted, labels=labels, normalize='pred')\n",
    "   plt.figure(figsize = (10,7))\n",
    "   sn.heatmap(cm, annot=False, xticklabels=labels, yticklabels=labels, cmap=\"Greens\", fmt='.1g')\n",
    "   \n",
    "   if outputfile != None:\n",
    "      with open(outputfile+\"_mae_clr.txt\", \"w\") as fd:\n",
    "         fd.write(str(mae))\n",
    "         fd.write(str(clr))\n",
    "      plt.savefig(outputfile+\"_cm.svg\")\n",
    "\n",
    "   return mae, clr, cm\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c4df5af-9c3e-4f82-8eb2-f50226907155",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "# Training\n",
    "def train(models, training_dir, ncf=NCF, verbose=False):\n",
    "   \"\"\" Function for training the classifiers \"\"\"\n",
    "\n",
    "   # Read training data\n",
    "   X_train, X_valid, y_train, y_valid = \\\n",
    "               read_data(training_dir, no_split=False, is_directory=True, verbose=verbose)\n",
    "\n",
    "   clfs = {}\n",
    "   for model in models:\n",
    "\n",
    "      if verbose: print(\" * Training \", model)\n",
    "      \n",
    "      # Create pipeline with several models using bag of words\n",
    "      if ncf == -1:\n",
    "         # without custom features\n",
    "         clf = Pipeline([\n",
    "            ('tfidf', TfidfVectorizer(stop_words=stop_words,\n",
    "                        ngram_range=(1, 2), use_idf=True)), \n",
    "            ('clf', create_model(model))\n",
    "         ])\n",
    "      else:\n",
    "         # with ncf custom features \n",
    "         clf = Pipeline([\n",
    "            ('features', FeatureUnion([\n",
    "               ('tfidf', TfidfVectorizer(stop_words=stop_words,\n",
    "                        ngram_range=(1, 2), use_idf=True)), \n",
    "               ('word_features', WordVectorizer(ncf))\n",
    "            ])),\n",
    "            ('clf', create_model(model))\n",
    "         ])\n",
    "\n",
    "      # Fitting model to training data\n",
    "      clf.fit(X_train.Text, y_train[\"Label\"])\n",
    "      if verbose: evaluate(clf, X_valid, y_valid[\"Label\"],\n",
    "               outputfile=f'train_results/{model}_{ncf}F')\n",
    "      with open('models/'+model+'_model.pckl', \"wb\") as f:\n",
    "         pickle.dump(clf, f)\n",
    "\n",
    "      clfs[model] = clf\n",
    "\n",
    "   return clfs\n",
    "\n",
    "\n",
    "# Test with all test files in all templates\n",
    "def test(clfs, test_dir, ncf=NCF, verbose=False):\n",
    "   \"\"\" Function for training the classifiers \"\"\"\n",
    "\n",
    "   # Read test data\n",
    "   X_test, y_test = read_data(test_dir, no_split=True, is_directory=True, verbose=verbose)\n",
    "\n",
    "   for model, clf in clfs.items():\n",
    "      print(\"Testing with\", model)\n",
    "      mae, clr, cm = evaluate(clf, X_test, y_test[\"Label\"],\n",
    "               outputfile=f'test_results/{model}_{ncf}F')\n",
    "   return mae, clr, cm\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "678b3fcd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train and test all the models\n",
    "\n",
    "clfs = train(models, \"train_files\", verbose=True)\n",
    "mae, clr, cm = test(clfs, \"test_files\", verbose=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c1d331d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test with each template separately\n",
    "\n",
    "def test_T(clfs, test_dir, ncf=NCF, verbose=False):\n",
    "    \"\"\" Function for training the classifiers \"\"\"\n",
    "\n",
    "    t = 1\n",
    "    results ={}\n",
    "    for filename in os.listdir(test_dir):\n",
    "        # Read test data in the template\n",
    "        X_test, y_test = read_data(test_dir+\"/\"+filename, no_split=True, is_directory=False, verbose=verbose)\n",
    "\n",
    "        for model, clf in clfs.items():\n",
    "            print(f\"Testing with {model} and Template {t}\")\n",
    "            mae, clr, cm = evaluate(clf, X_test, y_test[\"Label\"],\n",
    "                                    outputfile=f'test_results/{model}_T{t}_{ncf}F')\n",
    "            results[f\"T{t}\"] = (mae, clr, cm)\n",
    "        t+=1\n",
    "\n",
    "    return results\n",
    "    \n",
    "results = test_T(clfs, \"test_files\", verbose=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25e6b6db",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test changing the NCF (Number of Current Features) from 0 to 5\n",
    "\n",
    "train_files = \"train_files\"\n",
    "test_files = \"test_files\"\n",
    "\n",
    "for ncf in range(-1,NCF):\n",
    "    clfs = train(models, train_files, ncf=ncf, verbose=True)\n",
    "    mae, clr, cm = test(clfs, test_files, ncf=ncf, verbose=False)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
