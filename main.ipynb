{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b8625db0-cfe6-4c44-abfd-67a5fb134051",
   "metadata": {},
   "source": [
    "# Information Extraction from Electricity Invoices\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "94d449dc-924d-4357-8567-7c7bed016258",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, sys, pickle, time, uuid, shutil\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from ast import literal_eval\n",
    "from joblib import dump, load\n",
    "import re, subprocess\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sn\n",
    "\n",
    "# Constant definitions\n",
    "NWORDS = 11 #number of words per training sentence\n",
    "NCF = 5     #number of words for calculating custom features (maximum=NWORDS/2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af08be85-e164-4dda-aa80-a9354ef22c79",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Code for converting part of the training/test set into our (sentence,label) format\n",
    "\n",
    "from idsem2list import idsem_to_list\n",
    "\n",
    "generate_train = True\n",
    "nbills = 100\n",
    "\n",
    "# List template directories\n",
    "if generate_train:\n",
    "    base_dir = \"../dataset/idsem_txt/training/\"\n",
    "else:\n",
    "    base_dir = \"../dataset/idsem_txt/test/\"\n",
    "\n",
    "directories = os.listdir(base_dir)\n",
    "\n",
    "if generate_train:\n",
    "    out_dir = \"train_files_\" + str(nbills)\n",
    "else:\n",
    "    out_dir = \"test_files_\" + str(nbills)\n",
    "\n",
    "ext = 0\n",
    "for directory in directories:\n",
    "    ext = ext+1\n",
    "    train_data = idsem_to_list(base_dir+directory, nbills, out_dir, ext)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ed88af4-a853-4356-9553-6023c5a1dc8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Count the number of times that the labels appear in the documents\n",
    "\n",
    "import os\n",
    "import glob\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "def count_labels(file, n_labels):\n",
    "    '''Count the numer of times that each label appears in a file'''\n",
    "    \n",
    "    # read and tokenize text\n",
    "    text = open(file,\"r\", encoding=\"unicode_escape\").read()\n",
    "    text = word_tokenize(text) \n",
    "    ampersand = False\n",
    "    tmp = []\n",
    "    for t in text:\n",
    "        if t == \"#\":\n",
    "            ampersand = True\n",
    "        else:\n",
    "            if ampersand:\n",
    "                tmp.append(\"#\" + t)\n",
    "            else: \n",
    "                tmp.append(t)\n",
    "            ampersand = False\n",
    "            \n",
    "    text = tmp   \n",
    "\n",
    "    for k,v in n_labels.items():\n",
    "        n_labels[k].append(0)\n",
    "\n",
    "    for t in text:\n",
    "        try:\n",
    "            l = n_labels[t]\n",
    "            l[-1] = l[-1]+0.5 # adds 0.5 because each label appears twice \n",
    "        except KeyError:\n",
    "            pass\n",
    "    \n",
    "    return n_labels\n",
    "\n",
    "\n",
    "def count_all_labels(directory):\n",
    "\n",
    "    labels = [\"#A1\", \"#A2\", \"#A3\",  \"#A4\", \"#A5\", \"#A6\", \"#B1\", \"#B2\", \"#B3\", \"#B4\", \"#B5\", \"#B6\", \n",
    "              \"#C1\", \"#C2\", \"#C3\",  \"#C4\", \"#C5\", \"#C6\", \"#C7\", \"#C8\", \"#C9\", \"#CA\", \"#CB\", \"#CC\", \n",
    "              \"#CD\", \"#CE\", \"#D1\",  \"#D2\", \"#D9\", \"#DC\", \"#DD\", \"#E1\", \"#E2\", \"#E3\", \"#E3l\",\"#E4\", \n",
    "              \"#E5\", \"#E6\", \"#E7\",  \"#E7l\",\"#E7p\",\"#E7s\",\"#E8\", \"#E9\", \"#F1\", \"#F2\", \"#F3\", \"#F3l\", \n",
    "              \"#F3p\",\"#F3s\",\"#F4\",  \"#F4l\",\"#F4p\",\"#F4s\",\"#F4u\",\"#F5\", \"#F5l\",\"#F5p\",\"#F5s\",\"#F5u\",\n",
    "              \"#F6\", \"#F7\", \"#F8\",  \"#F8l\",\"#G1\", \"#G2\", \"#G3\", \"#G3l\",\"#G3p\",\"#G4\", \"#G5\", \"#G6\", \n",
    "              \"#G7\", \"#G8\", \"#G9\",  \"#GA\", \"#I1\", \"#I2\", \"#I3\", \"#J1\", \"#J2\", \"#J3\", \"#J4\", \"#J5\", \n",
    "              \"#K2\", \"#K2d\",\"#K3\",  \"#K4\", \"#K4d\",\"#K5\", \"#K6\", \"#K9\", \"#KA\", \"#KB\", \"#KC\", \"#KD\", \n",
    "              \"#M3\", \"#M3d\",\"#M4\",  \"#N1\", \"#N2\", \"#N3\", \"#N4\", \"#N5\", \"#N6\", \"#N7\", \"#N8\"]\n",
    "    \n",
    "    n_labels = dict(zip(labels, ([] for _ in labels)))\n",
    "\n",
    "    files = glob.glob(directory + '/**/Invoice0001_ann.txt', recursive=True)\n",
    "\n",
    "    for file in files:\n",
    "        count_labels(file, n_labels)\n",
    "        \n",
    "    [print(k[1:],int(sum(v)*5000)) for k,v in n_labels.items()]\n",
    "    \n",
    "count_all_labels(\"../dataset/idsem_txt/training\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "cbb71865-5f51-436d-966e-dd09a3ab257e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Use label codes for indexing the DataFrame\n",
    "label_codes = literal_eval(open(\"dictionaries/label_codes.txt\", mode=\"r\", encoding=\"utf-8\").read())\n",
    "\n",
    "# Create inverse mapping of label codes \n",
    "code_labels = {i: c for c,i in label_codes.items()}\n",
    "\n",
    "# Read spanish stop words\n",
    "stop_words = literal_eval(open(\"dictionaries/spanish_stop_words.txt\", mode=\"r\", encoding=\"utf-8\").read())\n",
    "stop_words = list(stop_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "23d4f93b-eff2-4b56-a765-7fa2abcf753a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "\n",
    "class WordVectorizer(TransformerMixin, BaseEstimator):\n",
    "   \"\"\"Extract features from the central word and its context\"\"\"\n",
    "   \n",
    "   def __init__(self, NCF=5):\n",
    "      self.NCF = NCF\n",
    "   \n",
    "   def word_type(self, S): \n",
    "      \"\"\"WordNType: alphabetic, numeric, alphanumeric, numericcomma\"\"\"\n",
    "      f = pd.Series(0, index=S.index)\n",
    "      f[S.str.isalnum()==True] = 1\n",
    "      f[S.str.isalpha()==True] = 2\n",
    "      f[S.str.isdigit()==True] = 3\n",
    "      f[S.str.contains('\\b[0-9,.]+$', regex=True)==True] = 4\n",
    "      return f\n",
    "   \n",
    "   def word_pretype(self, S): \n",
    "      \"\"\"WordNPretype: money, DNI, email, web, postcode\"\"\"\n",
    "      money = r'^\\d+[,.]\\d\\d$'\n",
    "      email = r\"^[a-zA-Z0-9.!#$%&'*+/=?^_`{|}~-]+@[a-zA-Z0-9](?:[a-zA-Z0-9-]{0,61}\"\\\n",
    "              \"[a-zA-Z0-9])?(?:\\.[a-zA-Z0-9](?:[a-zA-Z0-9-]{0,61}[a-zA-Z0-9])?)*$\"\n",
    "      web   = r\"^(https?:\\/\\/)?(www\\.)?([a-zA-Z0-9]+(-?[a-zA-Z0-9])*\\.)+[\\w]{2,}(\\/\\S*)?$\"\n",
    "      web2  = r\"^(http:\\/\\/www\\.|https:\\/\\/www\\.|http:\\/\\/|https:\\/\\/)?[a-z0-9]+([\\-\\.]{1}[a-z0-9]+)*\\.[a-z]{2,5}(:[0-9]{1,5})?(\\/.*)?$\"\n",
    "\n",
    "      dni1  = r'^[klmxyzKLMXYZ][-]?\\d{7}[-]?[a-zA-Z]$'\n",
    "      dni2  = r'^\\d{8}[-]?[a-zA-Z]$'\n",
    "      pcode = r\"^[0-5]\\d{4}$\"\n",
    "      date  = r\"^\\d\\d[/\\-.]\\d\\d[/\\-.]\\d{4}$\"\n",
    "      \n",
    "      f = pd.Series(0, index=S.index)\n",
    "      f[S.str.contains(money, regex=True)==True] = 1\n",
    "      f[S.str.contains(dni1,  regex=True)==True] = 2\n",
    "      f[S.str.contains(dni2,  regex=True)==True] = 2\n",
    "      f[S.str.contains(email, regex=True)==True] = 3\n",
    "      f[S.str.contains(web2,   regex=True)==True] = 4\n",
    "      f[S.str.contains(pcode, regex=True)==True] = 5\n",
    "      f[S.str.contains(date,  regex=True)==True] = 6\n",
    "      return f\n",
    "      \n",
    "   def word_mesure(self, S): \n",
    "      \"\"\"WordNMeasure: euro, euroday, eurokw, eurokwh, \n",
    "         kw, kwday, kwmonth, kwhour, %\"\"\"\n",
    "         \n",
    "      measures = {'€':1,      '€/día': 2,    '€/kw': 3,     '€/kwh': 4, \n",
    "                  'eur': 1,   'eur/día': 2,  'eur/kw': 3,   'eur/kwh': 4,\n",
    "                  'euros': 1, 'euros/día': 2,'euros/kw': 3, 'euros/kwh': 4,\n",
    "                  'kw': 6,    'kw/mes': 7,   'kwh': 8,      '%': 9}\n",
    "      L = S.str.lower()\n",
    "      f = L.map(measures)\n",
    "      f.fillna(0, inplace = True)\n",
    "      return f\n",
    "\n",
    "   def word_capital(self, S): \n",
    "      \"WordNCapital: firstlettercapital, onewordcapital, allcapital, nocapital\"\n",
    "      f = pd.Series(0, index=S.index) \n",
    "      f[S.str.contains(\"^[A-Z][a-z]+$\", regex=True)==True] = 1\n",
    "      f[S.str.contains(\"^[A-Z]$\", regex=True)==True] = 2\n",
    "      f[S.str.isupper()==True] = 3\n",
    "      f[S.str.islower()==True] = 4\n",
    "      f[S.str.istitle()==True] = 5\n",
    "      return f\n",
    " \n",
    "   def word_colons(self, S):    \n",
    "      \"WordNBeforeColons: 1 if it is followed by a semicolon or 0 otherwise\"\n",
    "      f = pd.Series(0, index=S.index) \n",
    "      f[S.str.contains(\":\")==True] = 1\n",
    "      f[S.str.contains(\";\")==True] = 2\n",
    "      f[S.str.contains(\".\")==True] = 3\n",
    "      return f\n",
    "\n",
    "   def fit(self, X, y=None):\n",
    "      return self\n",
    "\n",
    "   def transform(self, X, y=None):\n",
    "      \"\"\" Create custom features for a given word column in DataFrame\"\"\"\n",
    "      W = pd.DataFrame()\n",
    "      \n",
    "      if True:\n",
    "         # Use all the words in the training sentence\n",
    "         F = pd.DataFrame([t.split() for t in X], index = X.index)\n",
    "\n",
    "         print(\"NCF: \", self.NCF)\n",
    "         # Convert center word \n",
    "         if self.NCF>=0:\n",
    "            W[\"Type0\"]  = self.word_type(F[5])\n",
    "            W[\"PreType0\"]  = self.word_pretype(F[5])\n",
    "            W[\"Measure0\"]  = self.word_mesure(F[5])\n",
    "            W[\"Capital0\"]  = self.word_capital(F[5])\n",
    "            W[\"Colons0\"]  = self.word_colons(F[5])\n",
    "            \n",
    "            for i in range(self.NCF):\n",
    "               # Convert previous words\n",
    "               W[\"Type-\"+str(i+1)]  = self.word_type(F[5-i-1])\n",
    "               W[\"PreType-\"+str(i+1)]  = self.word_pretype(F[5-i-1])\n",
    "               W[\"Measure-\"+str(i+1)]  = self.word_mesure(F[5-i-1])\n",
    "               W[\"Capital-\"+str(i+1)]  = self.word_capital(F[5-i-1])\n",
    "               W[\"Colons-\"+str(i+1)]  = self.word_colons(F[5-i-1])\n",
    "               # Convert following words\n",
    "               W[\"Type+\"+str(i+1)]  = self.word_type(F[5+i+1])\n",
    "               W[\"PreType+\"+str(i+1)]  = self.word_pretype(F[5+i+1])\n",
    "               W[\"Measure+\"+str(i+1)]  = self.word_mesure(F[5+i+1])\n",
    "               W[\"Capital+\"+str(i+1)]  = self.word_capital(F[5+i+1])\n",
    "               W[\"Colons+\"+str(i+1)]  = self.word_colons(F[5+i+1])\n",
    "\n",
    "      return W\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9edc85fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import RidgeClassifier, LogisticRegression\n",
    "from sklearn.gaussian_process import GaussianProcessClassifier\n",
    "from sklearn.gaussian_process.kernels import RBF\n",
    "from sklearn.naive_bayes import MultinomialNB, GaussianNB\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.svm import SVC, LinearSVC\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import AdaBoostClassifier, RandomForestClassifier\n",
    "from sklearn.pipeline import Pipeline, FeatureUnion\n",
    "\n",
    "# Models \n",
    "def create_model(type):\n",
    "   \"\"\" Possible Models \"\"\"\n",
    "   model ={\n",
    "      \"ridge\": RidgeClassifier(alpha=0.01, class_weight='balanced', solver=\"sparse_cg\"),\n",
    "      \"logistic\": LogisticRegression(C=5, solver=\"saga\", tol=0.01),\n",
    "      \"KNN\": KNeighborsClassifier(n_neighbors=100), \n",
    "      \"nbayes\": MultinomialNB(alpha=0.01),\n",
    "      \"gaussprocess\": GaussianProcessClassifier(1.0 * RBF(1.0), random_state=42),\n",
    "      \"svmrbf\": SVC(gamma=0.001, C=100.0),\n",
    "      \"svmlin\": LinearSVC(random_state=0, class_weight=\"balanced\", tol=1e-5),\n",
    "      \"decisiontree\": DecisionTreeClassifier(random_state=42),\n",
    "      \"randomforest\": RandomForestClassifier(n_estimators=6, random_state=42, n_jobs=-1),\n",
    "      \"adaboost\": AdaBoostClassifier(algorithm=\"SAMME\", random_state=42),\n",
    "      \"MLP\": MLPClassifier(),\n",
    "   }\n",
    "\n",
    "   return model[type]\n",
    "\n",
    "all_models=[\"ridge\", \"logistic\", \"nbayes\", \"gaussprocess\", \"svmrbf\", \"svmlin\", \n",
    "            \"decisiontree\", \"randomforest\", \"adaboost\", \"MLP\", \"KNN\"]\n",
    "\n",
    "# models for testing\n",
    "models=[\"ridge\", \"nbayes\", \"logistic\", \"svmlin\",  \n",
    "        \"decisiontree\", \"randomforest\", \"svmrbf\" ]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7ff52e25-6116-4e93-afc7-ba0010cbdb90",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def process_input_file(file_bill, verbose=False):\n",
    "    \"\"\"Function that converts the PDF or image file to TXT\"\"\"\n",
    "\n",
    "    file_txt = 'tmp/' + str(uuid.uuid4())\n",
    "\n",
    "    file_type = os.path.splitext(file_bill)[1][1:].lower()\n",
    "\n",
    "    # convert input file to txt\n",
    "    if file_type == 'pdf':\n",
    "        #convert pdf to text\n",
    "        if verbose: print(\"Reading pdf file...\")\n",
    "        with open(file_bill,  \"rb\") as f:\n",
    "           pdf = pdftotext.PDF(f)\n",
    "        if verbose: print(\"Number of pages: \", len(pdf))\n",
    "\n",
    "        fd=open(file_txt + '.txt', \"w\");\n",
    "\n",
    "        if verbose: print(\"Converting to txt in \", file_txt + '.txt')\n",
    "\n",
    "        fd.write(\"\\n\".join(pdf))\n",
    "        fd.close()\n",
    "    else:\n",
    "        if file_type == 'txt':\n",
    "            #if a txt file just copy to destiny\n",
    "            shutil.copyfile(file_bill, file_txt + '.txt')\n",
    "        else:\n",
    "            #convert image to text\n",
    "            subprocess.call(['tesseract', file_bill, file_txt,\n",
    "                             '--oem', '1', '-l', 'spa'])\n",
    "\n",
    "    file_txt = file_txt + '.txt'\n",
    "    \n",
    "    lines = open(file_txt, 'r').readlines()\n",
    "    \n",
    "    os.remove(file_txt)\n",
    "    \n",
    "    return lines\n",
    "\n",
    "\n",
    "def read_file(filename, verbose=False):\n",
    "   \"\"\"\n",
    "   Read the content of one file\n",
    "   \"\"\"\n",
    "   l=[]\n",
    "   if verbose: print(\"Reading file \", filename)\n",
    "   with open(filename,'r', encoding=\"utf-8\") as f: \n",
    "         l=literal_eval(f.read())\n",
    "   return l\n",
    "   \n",
    "\n",
    "def read_directory(directory, verbose=False):\n",
    "   \"\"\"\n",
    "   Read the files in a directory and\n",
    "   inserts the data in a list\n",
    "   \"\"\"\n",
    "   X = []\n",
    "   if verbose: print(\"Accessing directory \", directory)\n",
    "   for filename in os.listdir(directory):\n",
    "      l=read_file(directory+\"/\"+filename, verbose)\n",
    "      X += l\n",
    "      #break\n",
    "         \n",
    "   if verbose: print('Training data contains ', len(X), ' samples')\n",
    "   \n",
    "   return X\n",
    "\n",
    "\n",
    "def create_dataframe(data):\n",
    "   \"\"\"\n",
    "   Function to create a DataFrame from a text training set \n",
    "   It processes word and line features\n",
    "   In this case there is only one label per line\n",
    "   \"\"\"\n",
    "   # Add columns for the text and each label in the DataFrame\n",
    "   F = pd.DataFrame(data, columns = [\"Text\", \"Label\"])\n",
    "\n",
    "   #extract the center and next words\n",
    "   a = F[\"Text\"].str.split(expand=True)\n",
    "   F[\"Word\"] = a[5]\n",
    "   F[\"NextWord\"] = a[6]\n",
    "\n",
    "   # remove # from the name of the labels\n",
    "   F[\"Label\"] = F[\"Label\"].apply(lambda x: x[1:])\n",
    "   F.index.names = ['Id']\n",
    "\n",
    "   return F\n",
    "\n",
    "\n",
    "def read_data(filename, no_split, is_directory=True, verbose=False):\n",
    "   ''' Read the data from the training dataset file\n",
    "       Separate the features from the labels\n",
    "       Separate dataset in train and validation sets\n",
    "   '''\n",
    "   \n",
    "   # Read the DataFrame as Text + Label\n",
    "   if is_directory:\n",
    "      X = read_directory(filename, verbose)\n",
    "   else:\n",
    "      X = read_file(filename, verbose)\n",
    "        \n",
    "   # Read DataFrame with one Label column\n",
    "   X = create_dataframe(X)\n",
    "\n",
    "   # Number and percentage of labels  \n",
    "   n = X['Label'].value_counts()\n",
    "   S = n.sum()\n",
    "\n",
    "   # Take out the label column and transform label names into codes\n",
    "   y = pd.DataFrame(X[\"Label\"], columns = [\"Label\"])\n",
    "   y = y.applymap(lambda x: label_codes[x]) \n",
    "      \n",
    "   # Remove label from feature set DataFrame\n",
    "   X.drop(\"Label\", axis=1, inplace=True)\n",
    "\n",
    "   # Encode labels with dictionary codes\n",
    "   y.index.rename('Id', inplace = True)\n",
    "   \n",
    "   if no_split:\n",
    "      return X, y\n",
    "   else:\n",
    "      X_train, X_valid, y_train, y_valid = train_test_split(\n",
    "         X, y, train_size=0.8, test_size=0.2, random_state=0, \n",
    "         shuffle=True\n",
    "      )\n",
    "\n",
    "      return X_train, X_valid, y_train, y_valid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1917f337",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import metrics\n",
    "\n",
    "def evaluate(clf, X, y, outputfile=None):\n",
    "   \"\"\" Evaluate the performance of a model with the validation/test set \"\"\"\n",
    "\n",
    "   # Preprocessing of validation data, get predictions\n",
    "   predicted = clf.predict(X.Text)\n",
    "   mae = metrics.mean_absolute_error(y, predicted)\n",
    "   print('  - MAE:', mae)\n",
    "\n",
    "   #metrics precision recall f1-score support\n",
    "   labels = y.unique()\n",
    "   index = [code_labels[i] for i in labels]\n",
    "\n",
    "   clr = metrics.classification_report(y, predicted,\n",
    "         labels=labels, target_names = index, digits=4)\n",
    "\n",
    "   labels=list(code_labels.keys())\n",
    "   cm = metrics.confusion_matrix(y, predicted, labels=labels, normalize='pred')\n",
    "   plt.figure(figsize = (10,7))\n",
    "   sn.heatmap(cm, annot=False, xticklabels=labels, yticklabels=labels, cmap=\"Greens\", fmt='.1g')\n",
    "   \n",
    "   if outputfile != None:\n",
    "      with open(outputfile+\"_mae_clr.txt\", \"w\") as fd:\n",
    "         fd.write(str(mae))\n",
    "         fd.write(str(clr))\n",
    "      plt.savefig(outputfile+\"_cm.svg\")\n",
    "\n",
    "   return mae, clr, cm\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2c4df5af-9c3e-4f82-8eb2-f50226907155",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "# Training\n",
    "def train(models, training_dir, ncf=NCF, verbose=False):\n",
    "   \"\"\" Function for training the classifiers \"\"\"\n",
    "\n",
    "   # Read training data\n",
    "   X_train, X_valid, y_train, y_valid = \\\n",
    "               read_data(training_dir, no_split=False, is_directory=True, verbose=verbose)\n",
    "\n",
    "   clfs = {}\n",
    "   for model in models:\n",
    "\n",
    "      if verbose: print(\" * Training \", model)\n",
    "      \n",
    "      # Create pipeline with several models using bag of words\n",
    "      if ncf == -1:\n",
    "         # without custom features\n",
    "         clf = Pipeline([\n",
    "            ('tfidf', TfidfVectorizer(stop_words=stop_words,\n",
    "                        ngram_range=(1, 2), use_idf=True)), \n",
    "            ('clf', create_model(model))\n",
    "         ])\n",
    "      else:\n",
    "         # with ncf custom features \n",
    "         clf = Pipeline([\n",
    "            ('features', FeatureUnion([\n",
    "               ('tfidf', TfidfVectorizer(stop_words=stop_words,\n",
    "                        ngram_range=(1, 2), use_idf=True)), \n",
    "               ('word_features', WordVectorizer(ncf))\n",
    "            ])),\n",
    "            ('clf', create_model(model))\n",
    "         ])\n",
    "\n",
    "      # Fitting model to training data\n",
    "      clf.fit(X_train.Text, y_train[\"Label\"])\n",
    "      if verbose: evaluate(clf, X_valid, y_valid[\"Label\"],\n",
    "               outputfile=f'train_results/{model}_{ncf}F')\n",
    "      with open('models/'+model+'_model.pckl', \"wb\") as f:\n",
    "         pickle.dump(clf, f)\n",
    "\n",
    "      clfs[model] = clf\n",
    "\n",
    "   return clfs\n",
    "\n",
    "\n",
    "# Test with all test files in all templates\n",
    "def test(clfs, test_dir, ncf=NCF, verbose=False):\n",
    "   \"\"\" Function for training the classifiers \"\"\"\n",
    "\n",
    "   # Read test data\n",
    "   X_test, y_test = read_data(test_dir, no_split=True, is_directory=True, verbose=verbose)\n",
    "\n",
    "   for model, clf in clfs.items():\n",
    "      print(\"Testing with\", model)\n",
    "      mae, clr, cm = evaluate(clf, X_test, y_test[\"Label\"],\n",
    "               outputfile=f'test_results/{model}_{ncf}F')\n",
    "   return mae, clr, cm\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "678b3fcd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accessing directory  train_files_50\n",
      "Reading file  train_files_50/T1.txt\n",
      "Reading file  train_files_50/train_data_T2.txt\n",
      "Reading file  train_files_50/train_data_T3.txt\n",
      "Reading file  train_files_50/train_data_T4.txt\n",
      "Reading file  train_files_50/train_data_T5.txt\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[9], line 3\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# Train and test all the models\u001b[39;00m\n\u001b[1;32m----> 3\u001b[0m clfs \u001b[38;5;241m=\u001b[39m \u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodels\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtrain_files_50\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[0;32m      4\u001b[0m mae, clr, cm \u001b[38;5;241m=\u001b[39m test(clfs, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtest_files_100\u001b[39m\u001b[38;5;124m\"\u001b[39m, verbose\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n",
      "Cell \u001b[1;32mIn[8], line 9\u001b[0m, in \u001b[0;36mtrain\u001b[1;34m(models, training_dir, ncf, verbose)\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\" Function for training the classifiers \"\"\"\u001b[39;00m\n\u001b[0;32m      7\u001b[0m \u001b[38;5;66;03m# Read training data\u001b[39;00m\n\u001b[0;32m      8\u001b[0m X_train, X_valid, y_train, y_valid \u001b[38;5;241m=\u001b[39m \\\n\u001b[1;32m----> 9\u001b[0m             \u001b[43mread_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtraining_dir\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mno_split\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mis_directory\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mverbose\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     11\u001b[0m clfs \u001b[38;5;241m=\u001b[39m {}\n\u001b[0;32m     12\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m model \u001b[38;5;129;01min\u001b[39;00m models:\n",
      "Cell \u001b[1;32mIn[6], line 97\u001b[0m, in \u001b[0;36mread_data\u001b[1;34m(filename, no_split, is_directory, verbose)\u001b[0m\n\u001b[0;32m     95\u001b[0m \u001b[38;5;66;03m# Read the DataFrame as Text + Label\u001b[39;00m\n\u001b[0;32m     96\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m is_directory:\n\u001b[1;32m---> 97\u001b[0m    X \u001b[38;5;241m=\u001b[39m \u001b[43mread_directory\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilename\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mverbose\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     98\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m     99\u001b[0m    X \u001b[38;5;241m=\u001b[39m read_file(filename, verbose)\n",
      "Cell \u001b[1;32mIn[6], line 59\u001b[0m, in \u001b[0;36mread_directory\u001b[1;34m(directory, verbose)\u001b[0m\n\u001b[0;32m     57\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m verbose: \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAccessing directory \u001b[39m\u001b[38;5;124m\"\u001b[39m, directory)\n\u001b[0;32m     58\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m filename \u001b[38;5;129;01min\u001b[39;00m os\u001b[38;5;241m.\u001b[39mlistdir(directory):\n\u001b[1;32m---> 59\u001b[0m    l\u001b[38;5;241m=\u001b[39m\u001b[43mread_file\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdirectory\u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m/\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43mfilename\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mverbose\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     60\u001b[0m    X \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m l\n\u001b[0;32m     61\u001b[0m    \u001b[38;5;66;03m#break\u001b[39;00m\n",
      "Cell \u001b[1;32mIn[6], line 47\u001b[0m, in \u001b[0;36mread_file\u001b[1;34m(filename, verbose)\u001b[0m\n\u001b[0;32m     45\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m verbose: \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mReading file \u001b[39m\u001b[38;5;124m\"\u001b[39m, filename)\n\u001b[0;32m     46\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mopen\u001b[39m(filename,\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124m'\u001b[39m, encoding\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mutf-8\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m f: \n\u001b[1;32m---> 47\u001b[0m       l\u001b[38;5;241m=\u001b[39m\u001b[43mliteral_eval\u001b[49m\u001b[43m(\u001b[49m\u001b[43mf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     48\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m l\n",
      "File \u001b[1;32mc:\\Users\\javier\\.conda\\envs\\svm-ebills\\lib\\ast.py:110\u001b[0m, in \u001b[0;36mliteral_eval\u001b[1;34m(node_or_string)\u001b[0m\n\u001b[0;32m    108\u001b[0m                 \u001b[38;5;28;01mreturn\u001b[39;00m left \u001b[38;5;241m-\u001b[39m right\n\u001b[0;32m    109\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m _convert_signed_num(node)\n\u001b[1;32m--> 110\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_convert\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnode_or_string\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\javier\\.conda\\envs\\svm-ebills\\lib\\ast.py:90\u001b[0m, in \u001b[0;36mliteral_eval.<locals>._convert\u001b[1;34m(node)\u001b[0m\n\u001b[0;32m     88\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mtuple\u001b[39m(\u001b[38;5;28mmap\u001b[39m(_convert, node\u001b[38;5;241m.\u001b[39melts))\n\u001b[0;32m     89\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(node, List):\n\u001b[1;32m---> 90\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mlist\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mmap\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m_convert\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnode\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43melts\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     91\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(node, Set):\n\u001b[0;32m     92\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mset\u001b[39m(\u001b[38;5;28mmap\u001b[39m(_convert, node\u001b[38;5;241m.\u001b[39melts))\n",
      "File \u001b[1;32mc:\\Users\\javier\\.conda\\envs\\svm-ebills\\lib\\ast.py:88\u001b[0m, in \u001b[0;36mliteral_eval.<locals>._convert\u001b[1;34m(node)\u001b[0m\n\u001b[0;32m     86\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m node\u001b[38;5;241m.\u001b[39mvalue\n\u001b[0;32m     87\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(node, Tuple):\n\u001b[1;32m---> 88\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mtuple\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mmap\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m_convert\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnode\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43melts\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     89\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(node, List):\n\u001b[0;32m     90\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mlist\u001b[39m(\u001b[38;5;28mmap\u001b[39m(_convert, node\u001b[38;5;241m.\u001b[39melts))\n",
      "File \u001b[1;32mc:\\Users\\javier\\.conda\\envs\\svm-ebills\\lib\\ast.py:85\u001b[0m, in \u001b[0;36mliteral_eval.<locals>._convert\u001b[1;34m(node)\u001b[0m\n\u001b[0;32m     84\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_convert\u001b[39m(node):\n\u001b[1;32m---> 85\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28;43misinstance\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mnode\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mConstant\u001b[49m\u001b[43m)\u001b[49m:\n\u001b[0;32m     86\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m node\u001b[38;5;241m.\u001b[39mvalue\n\u001b[0;32m     87\u001b[0m     \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(node, Tuple):\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Train and test all the models\n",
    "\n",
    "clfs = train(models, \"train_files\", verbose=True)\n",
    "mae, clr, cm = test(clfs, \"test_files\", verbose=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c1d331d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test with each template separately\n",
    "\n",
    "def test_T(clfs, test_dir, ncf=NCF, verbose=False):\n",
    "    \"\"\" Function for training the classifiers \"\"\"\n",
    "\n",
    "    t = 1\n",
    "    results ={}\n",
    "    for filename in os.listdir(test_dir):\n",
    "        # Read test data in the template\n",
    "        X_test, y_test = read_data(test_dir+\"/\"+filename, no_split=True, is_directory=False, verbose=verbose)\n",
    "\n",
    "        for model, clf in clfs.items():\n",
    "            print(f\"Testing with {model} and Template {t}\")\n",
    "            mae, clr, cm = evaluate(clf, X_test, y_test[\"Label\"],\n",
    "                                    outputfile=f'test_results/{model}_T{t}_{ncf}F')\n",
    "            results[f\"T{t}\"] = (mae, clr, cm)\n",
    "        t+=1\n",
    "\n",
    "    return results\n",
    "    \n",
    "results = test_T(clfs, \"test_files_100\", verbose=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25e6b6db",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test changing the NCF (Number of Current Features) from 0 to 5\n",
    "\n",
    "train_files = \"train_files_50\"\n",
    "test_files = \"test_files_100\"\n",
    "\n",
    "for ncf in range(-1,NCF):\n",
    "    clfs = train(models, train_files, ncf=ncf, verbose=True)\n",
    "    mae, clr, cm = test(clfs, test_files, ncf=ncf, verbose=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2bc428a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Divide the Confusion Matrix in six figures\n",
    "  \n",
    "n_labels = 18\n",
    "for i in range(6):\n",
    "    n1=i*n_labels\n",
    "    n2=(i+1)*n_labels\n",
    "    cm2 = cm[n1:n2,n1:n2]\n",
    "    keys = list(label_codes.keys())[n1:n2]\n",
    "    plt.figure(figsize=(10,7))\n",
    "    sn.heatmap(cm2, annot=False,xticklabels=keys,yticklabels=keys,cmap=\"Greens\", fmt='.1g')\n",
    "    plt.xlabel(\"Predicted\")\n",
    "    plt.ylabel(\"Actual\")\n",
    "    plt.savefig(f\"test_results/svm_cm{i}.svg\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d1ac6c2-4af5-4f5b-be04-cc1de7e961c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inference\n",
    "\n",
    "def predict(file_bill, verbose=False):\n",
    "   \"\"\" Function for predicting the fields of a given invoice \"\"\"\n",
    "   \n",
    "   # Process 11 words translated by one each time \n",
    "   bill_lines = process_input_file(file_bill, verbose)\n",
    "   bill_lines = ''.join(bill_lines)\n",
    "   bill_lines = bill_lines.replace(\"..\", \"\")\n",
    "   bill_lines = bill_lines.replace(\"\\n\", \" #eol \")\n",
    "   tokens = bill_lines.split()\n",
    "   bill_lines = [' '.join(tokens[i-5:i+5]) for i in range(5, len(tokens)-5)]\n",
    "\n",
    "   # Create feature vectors \n",
    "   X = pd.DataFrame({'Text': bill_lines})\n",
    "   \n",
    "   # Predict with SVM RBF model\n",
    "   if verbose: print(\" * SVM RBF method\")\n",
    "   with open(\"models/svmrbf_model.pckl\", \"rb\") as f:\n",
    "      model = pickle.load(f)\n",
    "\n",
    "   X[\"SVM\"] = model.predict(X.Text)\n",
    "   X[\"SVM\"] = X[\"SVM\"].map(lambda x: code_labels[x])\n",
    "\n",
    "   if verbose: X.to_csv('sresults.csv', index=False)\n",
    "      \n",
    "   # Convert to JSON\n",
    "   result = {}\n",
    "   labels = list(output[\"Label\"].unique())\n",
    "   labels.remove(\"OO\")\n",
    "   for label in labels:\n",
    "      t = output.Text[output[\"Label\"]==label]\n",
    "      words = [line.split()[int(len(line.split())/2)] for line in t]\n",
    "      words = [w for w in words if w != \"#eol\"]\n",
    "      result[label]= words\n",
    "      \n",
    "   if verbose: print(result)\n",
    "   \n",
    "   return result\n",
    "\n",
    "\n",
    "predict(\"../dataset/test/template1/Invoice0001.pdf\", True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
